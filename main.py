import os
import requests
import feedparser
from bs4 import BeautifulSoup
from openai import OpenAI
from supabase import create_client, Client
from dotenv import load_dotenv
import urllib.parse
import json
import re
import datetime

load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ë¡œì»¬ì—ì„œëŠ” .env, ì„œë²„ì—ì„œëŠ” Secretsì—ì„œ ë¡œë“œ)
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
supabase: Client = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_KEY"))
NAVER_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_SECRET = os.getenv("NAVER_CLIENT_SECRET")

# 1. ìˆ˜ì§‘ ëŒ€ìƒ ì„¤ì • (í‚¤ì›Œë“œ ë‹¤ì–‘í™” ë° RSS ì¶”ê°€)
KEYWORDS = [
    "ë°ì´í„°ì„¼í„° ì‹ ì¶• ìš©ëŸ‰", "ë°ì´í„°ì„¼í„° íŠ¹í™”ë‹¨ì§€", "ë°ì´í„°ì„¼í„° SMR", 
    "ë°ì´í„°ì„¼í„° ì•¡ì¹¨ëƒ‰ê°", "ë°ì´í„°ì„¼í„° ìˆ˜ì£¼"
]
RSS_FEEDS = [
    {"name": "DCD(Global)", "url": "https://www.datacenterdynamics.com/en/rss/"},
    {"name": "DCK(Global)", "url": "https://www.datacenterknowledge.com/rss.xml"},
    {"name": "ì „ìì‹ ë¬¸(KR)", "url": "https://www.etnews.com/etc/etnews_rss.html?igubun=0001"}
]

def scrape_article(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}
        res = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # ë³¸ë¬¸ ì¶”ì¶œ ë¡œì§ (ë‹¤ì–‘í•œ íŒ¨í„´ ì‹œë„)
        article = soup.select_one("#newsct_article") or soup.select_one(".article-body") or soup.select_one(".content-area") or soup.select_one("article")
        
        content = article.get_text(strip=True) if article else ""
        
        # 2ì°¨ ì•ˆì „ì¥ì¹˜: P íƒœê·¸ ê¸ì–´ì˜¤ê¸°
        if len(content) < 500:
            p_tags = soup.find_all('p')
            content = " ".join([p.get_text(strip=True) for p in p_tags if len(p.get_text(strip=True)) > 30])

        return content[:3500]
    except Exception as e:
        return ""

# main.pyì˜ analyze_ai í•¨ìˆ˜ ë‚´ë¶€ í”„ë¡¬í”„íŠ¸ë§Œ ìˆ˜ì •
def analyze_ai(text):
    # ì¢Œí‘œ ì¶”ì¶œì„ ë” ê°•ë ¥í•˜ê²Œ ìš”ì²­í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤.
    prompt = """
    Analyze the text as an energy analyst. Extract info in JSON format.
    Keep original language for text fields.
    Fields: 
    - project_name
    - location (City, State/Country)
    - lat (Approximate latitude of the location, float) <-- ì¢Œí‘œ ì¶”ì¶œì— ì§‘ì¤‘
    - lon (Approximate longitude of the location, float) <-- ì¢Œí‘œ ì¶”ì¶œì— ì§‘ì¤‘
    - power_capacity_mw
    - energy_tech
    - pue_target
    - companies
    
    If location is not a clear geographic place (like Moon), set lat/lon to null.
    If info is missing, use null.
    """
    try:
        # ... (ë‚˜ë¨¸ì§€ ì½”ë“œëŠ” ë™ì¼)
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "system", "content": prompt}, {"role": "user", "content": text}],
            response_format={ "type": "json_object" }
        )
        return json.loads(response.choices[0].message.content)
    except: return None

def process_and_save(title, link):
    # ì¤‘ë³µ ì²´í¬ (Supabase)
    check = supabase.table("projects").select("id").eq("url", link).execute()
    if not check.data:
        print(f"ğŸ” ë¶„ì„ ì‹œë„: {title[:30]}...")
        body = scrape_article(link)
        
        if len(body) > 100:
            analysis = analyze_ai(body)
            if analysis:
                # ë¦¬ìŠ¤íŠ¸ -> ë¬¸ìì—´ ë³€í™˜
                for k in analysis:
                    if isinstance(analysis[k], list): analysis[k] = ", ".join(map(str, analysis[k]))
                
                # ë°ì´í„° ì‚½ì…
                analysis.update({"title": title, "url": link})
                supabase.table("projects").insert(analysis).execute()
                print(f"âœ… ì €ì¥ ì™„ë£Œ!")
        else:
            print(f"âš ï¸ ìŠ¤í‚µ: ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ì§§ìŒ")

def main():
    print("ğŸš€ [1/2] ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ìˆ˜ì§‘ ì‹œì‘...")
    for kw in KEYWORDS:
        query = urllib.parse.quote(kw)
        url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display=5&sort=date" # 5ê°œì”©ìœ¼ë¡œ ì¤„ì„
        headers = {"X-Naver-Client-Id": NAVER_ID, "X-Naver-Client-Secret": NAVER_SECRET}
        try:
            items = requests.get(url, headers=headers).json().get('items', [])
            for item in items:
                process_and_save(item['title'].replace('<b>','').replace('</b>',''), item['link'])
        except: pass

    print("\nğŸš€ [2/2] ê¸€ë¡œë²Œ RSS í”¼ë“œ ìˆ˜ì§‘ ì‹œì‘...")
    for feed in RSS_FEEDS:
        print(f"ğŸ“¡ {feed['name']} ì ‘ì† ì¤‘...")
        try:
            parsed = feedparser.parse(feed['url'])
            for entry in parsed.entries[:5]: # ê° ë§¤ì²´ë‹¹ ìµœì‹  5ê°œ
                process_and_save(entry.title, entry.link)
        except Exception as e:
            print(f"RSS ì—ëŸ¬: {e}")

if __name__ == "__main__":
    # ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì‹œ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” ëŒ€ì‹  ì½˜ì†”ì— ì¶œë ¥
    if 'RUNNER_ENVIRONMENT' not in os.environ:
        print("\n--- ë¡œì»¬ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: DB ëŒ€ì‹  ì½˜ì†”ì— ì¶œë ¥ë©ë‹ˆë‹¤. ---")
        # (ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ DB ì €ì¥ ëŒ€ì‹  ì½˜ì†” ì¶œë ¥ ì½”ë“œë¥¼ ë„£ì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.)
        print("ë¡œì»¬ì—ì„œëŠ” DB ëŒ€ì‹  ì›¹ ëŒ€ì‹œë³´ë“œ í™•ì¸ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.")
    else:
        main()