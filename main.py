import os
import requests
import feedparser
from bs4 import BeautifulSoup
from openai import OpenAI
from supabase import create_client, Client
from dotenv import load_dotenv
import urllib.parse
import json

load_dotenv()

# ì„¤ì • ë¡œë“œ
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
supabase: Client = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_KEY"))
NAVER_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_SECRET = os.getenv("NAVER_CLIENT_SECRET")

# 1. ìˆ˜ì§‘ ëŒ€ìƒ ì„¤ì •
KEYWORDS = [
    "ë°ì´í„°ì„¼í„° ì‹ ì¶• ìš©ëŸ‰",
    "ë°ì´í„°ì„¼í„° íŠ¹í™”ë‹¨ì§€",
    "ë°ì´í„°ì„¼í„° SMR ì›ì „",
    "ë°ì´í„°ì„¼í„° ì•¡ì¹¨ëƒ‰ê°",
    "ë°ì´í„°ì„¼í„° ìˆ˜ì£¼"
]

RSS_FEEDS = [
    {"name": "ì „ìì‹ ë¬¸(IT)", "url": "https://www.etnews.com/etc/etnews_rss.html?igubun=0001"},
    {"name": "DCD(Global)", "url": "https://www.datacenterdynamics.com/en/rss/"} # ì™¸êµ­ ê¸°ì‚¬ ì¶”ê°€
]

def scrape_article(url):
    try:
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=7)
        soup = BeautifulSoup(res.text, 'html.parser')
        # ë§¤ì²´ë³„ ë³¸ë¬¸ íƒœê·¸ ëŒ€ì‘ (ë„¤ì´ë²„, DCD ë“±)
        article = soup.select_one("#newsct_article") or soup.select_one(".article-body") or soup.select_one("article")
        return article.get_text(strip=True)[:2500] if article else ""
    except: return ""

def analyze_ai(text, lang="ko"):
    # ì™¸êµ­ ê¸°ì‚¬ì¼ ê²½ìš° í•œê¸€ë¡œ ë²ˆì—­í•˜ì—¬ ì¶”ì¶œí•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
    prompt = f"ë„ˆëŠ” ê¸€ë¡œë²Œ ì—ë„ˆì§€ ë¶„ì„ê°€ì•¼. ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ JSONìœ¼ë¡œ ì‘ë‹µí•´. ëª¨ë“  ê°’ì€ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì„œ ì‘ì„±í•´. í•„ë“œ: project_name, location, power_capacity_mw, energy_tech, pue_target, companies. ì—†ìœ¼ë©´ null."
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "system", "content": prompt}, {"role": "user", "content": text}],
            response_format={ "type": "json_object" }
        )
        return json.loads(response.choices[0].message.content)
    except: return None

def process_and_save(title, link):
    # ì¤‘ë³µ ì²´í¬
    check = supabase.table("projects").select("id").eq("url", link).execute()
    if not check.data:
        body = scrape_article(link)
        if len(body) > 300:
            analysis = analyze_ai(body)
            if analysis:
                for k in analysis:
                    if isinstance(analysis[k], list): analysis[k] = ", ".join(map(str, analysis[k]))
                analysis.update({"title": title, "url": link})
                supabase.table("projects").insert(analysis).execute()
                print(f"âœ… ì €ì¥: {title[:30]}...")

def main():
    print("ğŸš€ [1/2] ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ìˆ˜ì§‘ ì‹œì‘ (Keyword-based)...")
    for kw in KEYWORDS:
        query = urllib.parse.quote(kw)
        url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display=10&sort=date"
        headers = {"X-Naver-Client-Id": NAVER_ID, "X-Naver-Client-Secret": NAVER_SECRET}
        items = requests.get(url, headers=headers).json().get('items', [])
        for item in items:
            process_and_save(item['title'].replace('<b>','').replace('</b>',''), item['link'])

    print("\nğŸš€ [2/2] RSS í”¼ë“œ ìˆ˜ì§‘ ì‹œì‘ (Source-based)...")
    for feed in RSS_FEEDS:
        print(f"ğŸ“¡ {feed['name']} ì½ëŠ” ì¤‘...")
        parsed = feedparser.parse(feed['url'])
        for entry in parsed.entries[:10]: # ê° ë§¤ì²´ë‹¹ ìµœì‹  10ê°œ
            process_and_save(entry.title, entry.link)

if __name__ == "__main__":
    main()