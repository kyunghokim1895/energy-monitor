import os
import requests
import feedparser
from bs4 import BeautifulSoup
from openai import OpenAI
from supabase import create_client, Client
from dotenv import load_dotenv
import urllib.parse
import json
import re

load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
supabase: Client = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_KEY"))
NAVER_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_SECRET = os.getenv("NAVER_CLIENT_SECRET")

# ìˆ˜ì§‘ í‚¤ì›Œë“œ ë° RSS ëª©ë¡
KEYWORDS = ["ë°ì´í„°ì„¼í„° ì‹ ì¶•", "ë°ì´í„°ì„¼í„° ì „ë ¥", "ë°ì´í„°ì„¼í„° ìˆ˜ì£¼"]
RSS_FEEDS = [
    # ê¸€ë¡œë²Œ ë¦¬ë”© ë§¤ì²´ (DCD)
    {"name": "DCD", "url": "https://www.datacenterdynamics.com/en/rss/"},
    # ë°ì´í„°ì„¼í„° ì§€ì‹ (Global)
    {"name": "DCK", "url": "https://www.datacenterknowledge.com/rss.xml"},
    # êµ­ë‚´ ì „ë¬¸ì§€
    {"name": "ì „ìì‹ ë¬¸", "url": "https://www.etnews.com/etc/etnews_rss.html?igubun=0001"}
]

def scrape_article(url):
    try:
        # ë´‡ ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ í—¤ë” ì„¤ì •
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        res = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        
        content = ""
        
        # 1ì°¨ ì‹œë„: ì£¼ìš” ë‰´ìŠ¤ ì‚¬ì´íŠ¸ë“¤ì˜ ë³¸ë¬¸ íƒœê·¸ íŒ¨í„´ íƒìƒ‰
        selectors = [
            "#newsct_article", ".article-body", ".content-body", 
            "article", ".post-content", ".story-body", "#article-view-content-div"
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                content = element.get_text(strip=True)
                break
        
        # 2ì°¨ ì‹œë„ (ì•ˆì „ì¥ì¹˜): ë§Œì•½ ìœ„ì—ì„œ ëª» ì°¾ì•˜ìœ¼ë©´, ê·¸ëƒ¥ ëª¨ë“  <p> íƒœê·¸ë¥¼ ê¸ì–´ì˜´
        if len(content) < 50:
            p_tags = soup.find_all('p')
            # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥(ë©”ë‰´ ë“±)ì€ ì œì™¸í•˜ê³  ë³¸ë¬¸ ê°™ì€ ê²ƒë§Œ í•©ì¹¨
            content = " ".join([p.get_text(strip=True) for p in p_tags if len(p.get_text(strip=True)) > 30])

        return content[:3500] # ë„ˆë¬´ ê¸¸ë©´ ìë¦„
    except Exception as e:
        print(f"âŒ ìŠ¤í¬ë˜í•‘ ì—ëŸ¬: {e}")
        return ""

def analyze_ai(text):
    # ì˜ë¬¸ì€ ì˜ë¬¸ ê·¸ëŒ€ë¡œ, í•œê¸€ì€ í•œê¸€ ê·¸ëŒ€ë¡œ ì¶”ì¶œ ìš”ì²­
    prompt = """
    Analyze the text as an energy analyst. Extract info in JSON format.
    Keep original language (English->English, Korean->Korean).
    Fields: project_name, location, power_capacity_mw, energy_tech, pue_target, companies.
    If info is missing, use null.
    """
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "system", "content": prompt}, {"role": "user", "content": text}],
            response_format={ "type": "json_object" }
        )
        return json.loads(response.choices[0].message.content)
    except: return None

def process_and_save(title, link):
    # ì¤‘ë³µ ì²´í¬
    check = supabase.table("projects").select("id").eq("url", link).execute()
    if not check.data:
        print(f"ğŸ” ë¶„ì„ ì‹œë„: {title[:30]}...")
        body = scrape_article(link)
        
        # ë³¸ë¬¸ì´ 100ì ì´ìƒì¼ ë•Œë§Œ ë¶„ì„ (ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ)
        if len(body) > 100:
            analysis = analyze_ai(body)
            if analysis:
                # ë¦¬ìŠ¤íŠ¸ -> ë¬¸ìì—´ ë³€í™˜
                for k in analysis:
                    if isinstance(analysis[k], list): analysis[k] = ", ".join(map(str, analysis[k]))
                
                analysis.update({"title": title, "url": link})
                supabase.table("projects").insert(analysis).execute()
                print(f"âœ… ì €ì¥ ì™„ë£Œ!")
        else:
            print(f"âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ (ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŒ)")

def main():
    print("ğŸš€ [1/2] ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰...")
    for kw in KEYWORDS:
        query = urllib.parse.quote(kw)
        url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display=5&sort=date"
        headers = {"X-Naver-Client-Id": NAVER_ID, "X-Naver-Client-Secret": NAVER_SECRET}
        try:
            items = requests.get(url, headers=headers).json().get('items', [])
            for item in items:
                process_and_save(item['title'].replace('<b>','').replace('</b>',''), item['link'])
        except: pass

    print("\nğŸš€ [2/2] ê¸€ë¡œë²Œ RSS í”¼ë“œ ìˆ˜ì§‘...")
    for feed in RSS_FEEDS:
        print(f"ğŸ“¡ {feed['name']} ì ‘ì† ì¤‘...")
        try:
            parsed = feedparser.parse(feed['url'])
            # ìµœì‹  ê¸€ 5ê°œì”©ë§Œ í™•ì¸
            for entry in parsed.entries[:5]:
                process_and_save(entry.title, entry.link)
        except Exception as e:
            print(f"RSS ì—ëŸ¬: {e}")

if __name__ == "__main__":
    main()