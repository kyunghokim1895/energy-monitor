import streamlit as st
import sqlite3
import pandas as pd
import requests
from bs4 import BeautifulSoup
from openai import OpenAI
import urllib.parse
import os

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ì—ë„ˆì§€/ë°ì´í„°ì„¼í„° ëª¨ë‹ˆí„°ë§", layout="wide")

# 1. API í‚¤ ì„¤ì • (Streamlit Secrets ì‚¬ìš©)
# ë¡œì»¬ì—ì„œëŠ” .envë¥¼ ì“°ì§€ë§Œ, ì„œë²„ì—ì„œëŠ” st.secretsë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.
try:
    NAVER_ID = st.secrets["NAVER_CLIENT_ID"]
    NAVER_SECRET = st.secrets["NAVER_CLIENT_SECRET"]
    OPENAI_KEY = st.secrets["OPENAI_API_KEY"]
except:
    # ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš© (Secretsê°€ ì—†ì„ ë•Œ .env ë¡œë“œ)
    from dotenv import load_dotenv
    load_dotenv()
    NAVER_ID = os.getenv("NAVER_CLIENT_ID")
    NAVER_SECRET = os.getenv("NAVER_CLIENT_SECRET")
    OPENAI_KEY = os.getenv("OPENAI_API_KEY")

client = OpenAI(api_key=OPENAI_KEY)

# 2. ë°ì´í„°ë² ì´ìŠ¤ ë° í…Œì´ë¸” ì´ˆê¸°í™” í•¨ìˆ˜
def init_db():
    conn = sqlite3.connect("energy_data.db")
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS projects (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT, url TEXT UNIQUE, project_name TEXT,
            location TEXT, power_capacity_mw TEXT,
            energy_tech TEXT, pue_target TEXT, companies TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    conn.commit()
    return conn

# --- ìˆ˜ì§‘ ë¡œì§ (ê¸°ì¡´ main.py ë‚´ìš© í•©ì¹¨) ---
def scrape_article(url):
    try:
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        article = soup.select_one("#newsct_article") or soup.select_one("#articleBodyContents") or soup.select_one("article")
        return article.get_text(strip=True)[:2000] if article else ""
    except: return ""

def analyze_ai(text):
    prompt = "ì—ë„ˆì§€ ë¶„ì„ê°€ë¡œì„œ project_name, location, power_capacity_mw, energy_tech, pue_target, companies ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì¶”ì¶œí•´. ì—†ìœ¼ë©´ null."
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "system", "content": prompt}, {"role": "user", "content": text}],
        response_format={ "type": "json_object" }
    )
    import json
    return json.loads(response.choices[0].message.content)

# 3. ë©”ì¸ í™”ë©´
st.title("âš¡ ì‹¤ì‹œê°„ ì—ë„ˆì§€/ë°ì´í„°ì„¼í„° ëª¨ë‹ˆí„°ë§")

# ì‚¬ì´ë“œë°” - ë°ì´í„° ìˆ˜ì§‘ ê¸°ëŠ¥
st.sidebar.header("ğŸ•¹ï¸ ì»¨íŠ¸ë¡¤ íŒ¨ë„")
if st.sidebar.button("ğŸ” ìµœì‹  ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘"):
    with st.spinner("ë‰´ìŠ¤ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
        conn = init_db()
        cursor = conn.cursor()
        
        # ë‰´ìŠ¤ ê²€ìƒ‰
        query = urllib.parse.quote("ë°ì´í„°ì„¼í„° ì‹ ì¶• ìš©ëŸ‰")
        url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display=5&sort=date"
        headers = {"X-Naver-Client-Id": NAVER_ID, "X-Naver-Client-Secret": NAVER_SECRET}
        items = requests.get(url, headers=headers).json().get('items', [])
        
        new_count = 0
        for item in items:
            link = item['link']
            cursor.execute("SELECT id FROM projects WHERE url = ?", (link,))
            if not cursor.fetchone():
                body = scrape_article(link)
                if len(body) > 200:
                    analysis = analyze_ai(body)
                    # ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                    for k in analysis:
                        if isinstance(analysis[k], list): analysis[k] = ", ".join(map(str, analysis[k]))
                    
                    cursor.execute("""
                        INSERT INTO projects (title, url, project_name, location, power_capacity_mw, energy_tech, pue_target, companies)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    """, (item['title'], link, analysis.get('project_name'), analysis.get('location'),
                          analysis.get('power_capacity_mw'), analysis.get('energy_tech'),
                          analysis.get('pue_target'), analysis.get('companies')))
                    new_count += 1
        conn.commit()
        conn.close()
        st.sidebar.success(f"{new_count}ê°œì˜ ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
        st.rerun()

# 4. ë°ì´í„° í‘œì‹œ ë¶€ë¶„
conn = init_db() # í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±
df = pd.read_sql("SELECT * FROM projects ORDER BY created_at DESC", conn)
conn.close()

if not df.empty:
    st.metric("ì´ ìˆ˜ì§‘ í”„ë¡œì íŠ¸", f"{len(df)}ê±´")
    st.dataframe(df.drop(columns=['id']), use_container_width=True)
else:
    st.warning("í˜„ì¬ ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì™¼ìª½ì˜ 'ìµœì‹  ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”!")